# âš¡ Battery RL Environment

A lightweight yet extendable **Reinforcement Learning project** for optimizing **battery energy storage** using real-world electricity prices.  
Implements the **Gymnasium** API for single-agent training and is being extended into a **multi-agent environment** using **PettingZoo**.  
Built for learning and experimentation with **Stable-Baselines3** and **MARL frameworks**.

---

## ğŸ“˜ Overview

This repo contains:
- **Single-agent environment** (`BatteryEnv`) in `envs/single_agent.py`
- **Training scripts** (PPO, SAC) under `scripts/single_agent/`
- **Evaluation** tools comparing trained vs. random policies
- A **multi-agent extension** (in progress) in `envs/multi_agent.py`
- Real electricity **price datasets** under `datasets/`

---

## ğŸš€ Features

âœ… Standard **Gymnasium interface** (`reset`, `step`, `render`)  
âœ… Works directly with **Stable-Baselines3**  
âœ… Supports **continuous** (`[-1, 1]`) or **discrete** (`{0=discharge, 1=idle, 2=charge}`) actions  
âœ… Observations: battery **state-of-charge** and **current price**  
âœ… Reward = **profit** from buying/selling electricity  
âœ… Configurable (episode length, capacities, rates, efficiencies, etc.)  
âœ… Fully reproducible via seeding  
âœ… Clean structure for scaling to **multi-agent** (PettingZoo-ready)

---

## ğŸ§© Project Structure

```
battery-rl-env/
â”œâ”€ envs/
â”‚  â”œâ”€ single_agent.py          # BatteryEnv (Gymnasium)
â”‚  â””â”€ multi_agent.py           # WIP: MARL version (PettingZoo)
â”œâ”€ scripts/
â”‚  â”œâ”€ single_agent/
â”‚  â”‚  â”œâ”€ train_ppo.py          # Train PPO agent
â”‚  â”‚  â”œâ”€ train_sac.py          # Train SAC agent
â”‚  â”‚  â””â”€ eval.py               # Evaluate trained vs random agent
â”‚  â””â”€ multi_agent/
â”‚     â”œâ”€ train_independent.py  # (planned) Independent multi-agent training
â”‚     â””â”€ eval.py               # (planned) MARL evaluation
â”œâ”€ utils/
â”‚  â”œâ”€ pricing.py               # load/validate price series
â”‚  â”œâ”€ metrics.py               # profit summaries & win-rate
â”‚  â””â”€ seeding.py               # reproducible RNG helpers
â”œâ”€ datasets/
â”‚  â”œâ”€ energy_prices_2023_france.csv
â”‚  â””â”€ energy_prices_2024_france.csv
â””â”€ tests/                      # small sanity checks
```

---

## ğŸ§  Planned Work
- âœ… Train & evaluate **PPO** and **SAC** agents on real datasets  
- ğŸ§© Implement **multi-agent (PettingZoo)** version  
- ğŸ“ˆ Add logging, normalization, and richer evaluation metrics  
- ğŸ” Experiment with **shared-grid / price-coupled MARL setups**

---

## ğŸ’» Quick Example

```python
import numpy as np
from envs.single_agent import BatteryEnv

# Random price series for demo
price_series = np.random.uniform(50, 150, size=1000)

env = BatteryEnv(price_series=price_series, continuous_action=True)
obs, info = env.reset()

done = False
while not done:
    action = env.action_space.sample()
    obs, reward, terminated, truncated, info = env.step(action)
    done = terminated or truncated
    print(obs, reward)
```

---

## ğŸ“š Requirements
```
gymnasium
numpy
pandas
stable-baselines3
pettingzoo      # (for future MARL)
```

Install with:
```bash
pip install -r requirements.txt
```
